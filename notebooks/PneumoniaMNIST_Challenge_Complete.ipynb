{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {"provenance": [], "gpuType": "T4", "name": "PneumoniaMNIST_Challenge_Complete.ipynb"},
  "kernelspec": {"name": "python3", "display_name": "Python 3"},
  "language_info": {"name": "python"},
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü©∫ Postdoctoral Technical Challenge ‚Äî PneumoniaMNIST\n",
    "**AlfaisalX: Cognitive Robotics and Autonomous Agents**  \n",
    "**MedX Research Unit, Alfaisal University, Riyadh, Saudi Arabia**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is **100% self-contained** ‚Äî all code is defined inline, no repository clone or local imports needed.\n",
    "Run cells top to bottom on Colab free tier (T4 GPU recommended).\n",
    "\n",
    "| Task | Method | Key Tech |\n",
    "|------|---------|----------|\n",
    "| **Task 1** | CNN Classification + full evaluation | EfficientNet-B0, Focal Loss, AdamW |\n",
    "| **Task 2** | Medical Report Generation | MedGemma-4B-IT VLM |\n",
    "| **Task 3** | Semantic Image Retrieval | BioMedCLIP + FAISS |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## ‚öôÔ∏è 0. Install & Setup"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q medmnist timm open-clip-torch faiss-cpu seaborn scikit-learn\n",
    "!pip install -q transformers accelerate\n",
    "print('‚úì Dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, warnings, copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42); np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB')\n",
    "\n",
    "for d in ['outputs/task1','outputs/task2','outputs/task3']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "CLASS_NAMES = ['Normal', 'Pneumonia']\n",
    "print('‚úì Setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã Task 1: CNN Classification"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 1.1 Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import PneumoniaMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "IMAGE_SIZE, BATCH_SIZE = 224, 32\n",
    "\n",
    "# Medical X-ray augmentation rules:\n",
    "#  ‚úì Horizontal flip  ‚Äî mirror anatomy is valid\n",
    "#  ‚úì Small rotation   ‚Äî patient positioning variation\n",
    "#  ‚úó Vertical flip    ‚Äî NEVER: produces anatomically invalid images\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "eval_tfm = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "vis_tfm = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = PneumoniaMNIST(split='train', transform=train_tfm, download=True, as_rgb=True)\n",
    "val_ds   = PneumoniaMNIST(split='val',   transform=eval_tfm,  download=True, as_rgb=True)\n",
    "test_ds  = PneumoniaMNIST(split='test',  transform=eval_tfm,  download=True, as_rgb=True)\n",
    "vis_ds   = PneumoniaMNIST(split='test',  transform=vis_tfm,   download=True, as_rgb=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "train_labels = [train_ds[i][1].item() for i in range(len(train_ds))]\n",
    "n0, n1 = train_labels.count(0), train_labels.count(1)\n",
    "print(f'Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}')\n",
    "print(f'Class ‚Äî Normal: {n0} ({100*n0/len(train_labels):.1f}%) | Pneumonia: {n1} ({100*n1/len(train_labels):.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset visualization\n",
    "test_labels_list = [test_ds[i][1].item() for i in range(len(test_ds))]\n",
    "fig, axes = plt.subplots(2, 8, figsize=(18, 5))\n",
    "fig.suptitle('PneumoniaMNIST Samples', fontsize=14, fontweight='bold')\n",
    "for cls_idx, cls_name in enumerate(CLASS_NAMES):\n",
    "    samples = [i for i, l in enumerate(test_labels_list) if l == cls_idx][:8]\n",
    "    for j, idx in enumerate(samples):\n",
    "        img, _ = vis_ds[idx]\n",
    "        axes[cls_idx, j].imshow(img.permute(1,2,0).numpy()[:,:,0], cmap='gray')\n",
    "        axes[cls_idx, j].set_title(cls_name, fontsize=8,\n",
    "            color='steelblue' if cls_idx==0 else 'tomato')\n",
    "        axes[cls_idx, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/task1/dataset_samples.png', dpi=130, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model: EfficientNet-B0 + Focal Loss\n",
    "- **EfficientNet-B0**: 5.3M params, compound scaling (depth/width/resolution), ImageNet pretrained\n",
    "- **Focal Loss** `Œ≥=2.0`: Down-weights easy examples, crucial for ~74% pneumonia imbalance\n",
    "- **AdamW + cosine warmup**: Weight decay prevents overfitting on small dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"FL = -Œ±(1-p_t)^Œ≥ log(p_t). Handles class imbalance better than CE.\"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.gamma, self.alpha = gamma, alpha\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce)\n",
    "        return (self.alpha * (1-pt)**self.gamma * ce).mean()\n",
    "\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=2, in_chans=3).to(device)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'EfficientNet-B0 | Parameters: {total:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 1.3 Training"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "LR, WD = 1e-4, 1e-4\n",
    "WARMUP = 3\n",
    "\n",
    "criterion = FocalLoss(gamma=2.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "def lr_lambda(ep):\n",
    "    if ep < WARMUP: return (ep+1)/WARMUP\n",
    "    return 0.5*(1 + np.cos(np.pi*(ep-WARMUP)/max(NUM_EPOCHS-WARMUP,1)))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def run_epoch(model, loader, opt=None):\n",
    "    training = opt is not None\n",
    "    model.train() if training else model.eval()\n",
    "    tot_loss, correct, total = 0., 0, 0\n",
    "    ctx = torch.enable_grad() if training else torch.no_grad()\n",
    "    with ctx:\n",
    "        for imgs, lbls in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            lbls = lbls.squeeze().long().to(device)\n",
    "            if training: opt.zero_grad()\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, lbls)\n",
    "            if training:\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                opt.step()\n",
    "            tot_loss += loss.item()*imgs.size(0)\n",
    "            correct += (out.argmax(1)==lbls).sum().item()\n",
    "            total += lbls.size(0)\n",
    "    return tot_loss/total, correct/total\n",
    "\n",
    "history = {'train_loss':[],'val_loss':[],'train_acc':[],'val_acc':[]}\n",
    "best_val_acc, best_epoch, best_state = 0., 0, None\n",
    "\n",
    "for ep in range(1, NUM_EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_l, tr_a = run_epoch(model, train_loader, optimizer)\n",
    "    va_l, va_a = run_epoch(model, val_loader)\n",
    "    scheduler.step()\n",
    "    for k, v in zip(['train_loss','val_loss','train_acc','val_acc'],[tr_l,va_l,tr_a,va_a]):\n",
    "        history[k].append(v)\n",
    "    if va_a > best_val_acc:\n",
    "        best_val_acc, best_epoch = va_a, ep\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "    print(f'Ep {ep:02d}/{NUM_EPOCHS} | Train {tr_l:.4f}/{tr_a:.4f} | '\n",
    "          f'Val {va_l:.4f}/{va_a:.4f} | {time.time()-t0:.1f}s{\" ‚òÖ\" if va_a==best_val_acc else \"\"}')\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "torch.save(best_state, 'outputs/task1/best_model.pth')\n",
    "with open('outputs/task1/history.json','w') as f:\n",
    "    json.dump({**history,'best_epoch':best_epoch,'best_val_acc':best_val_acc}, f)\n",
    "print(f'\\n‚úì Best val acc: {best_val_acc:.4f} at epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 1.4 Evaluation"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, classification_report\n",
    ")\n",
    "\n",
    "# Training curves\n",
    "eps = range(1, NUM_EPOCHS+1)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(eps, history['train_loss'], label='Train', lw=2, color='#2196F3')\n",
    "axes[0].plot(eps, history['val_loss'],   label='Val',   lw=2, color='#F44336')\n",
    "axes[0].axvline(best_epoch, ls='--', color='gray', label=f'Best ep={best_epoch}')\n",
    "axes[0].set(title='Loss', xlabel='Epoch', ylabel='Loss'); axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "axes[1].plot(eps, [a*100 for a in history['train_acc']], label='Train', lw=2, color='#2196F3')\n",
    "axes[1].plot(eps, [a*100 for a in history['val_acc']],   label='Val',   lw=2, color='#F44336')\n",
    "axes[1].axvline(best_epoch, ls='--', color='gray', label=f'Best ep={best_epoch}')\n",
    "axes[1].set(title='Accuracy', xlabel='Epoch', ylabel='Acc (%)'); axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/task1/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Test predictions\n",
    "model.eval()\n",
    "y_true, y_pred, y_prob = [], [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in test_loader:\n",
    "        out = model(imgs.to(device))\n",
    "        probs = torch.softmax(out,1)[:,1].cpu().numpy()\n",
    "        preds = out.argmax(1).cpu().numpy()\n",
    "        y_true.extend(lbls.squeeze().numpy())\n",
    "        y_pred.extend(preds); y_prob.extend(probs)\n",
    "y_true, y_pred, y_prob = map(np.array, [y_true, y_pred, y_prob])\n",
    "\n",
    "metrics = {\n",
    "    'accuracy':  float(accuracy_score(y_true, y_pred)),\n",
    "    'precision': float(precision_score(y_true, y_pred)),\n",
    "    'recall':    float(recall_score(y_true, y_pred)),\n",
    "    'f1':        float(f1_score(y_true, y_pred)),\n",
    "    'auc':       float(roc_auc_score(y_true, y_prob)),\n",
    "}\n",
    "with open('outputs/task1/test_metrics.json','w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print('=== TEST SET METRICS ===')\n",
    "for k, v in metrics.items(): print(f'  {k:10s}: {v:.4f}')\n",
    "print(); print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "            ax=axes[0], annot_kws={'size':14})\n",
    "axes[0].set(xlabel='Predicted', ylabel='True', title='Confusion Matrix')\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "axes[1].plot(fpr, tpr, lw=2, color='#2196F3', label=f'AUC={metrics[\"auc\"]:.4f}')\n",
    "axes[1].plot([0,1],[0,1],'k--',lw=1.5); axes[1].fill_between(fpr,tpr,alpha=0.1,color='#2196F3')\n",
    "axes[1].set(xlabel='FPR',ylabel='TPR',title='ROC Curve'); axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/task1/confusion_roc.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure cases ‚Äî collect all vis_ds predictions\n",
    "vis_loader = DataLoader(vis_ds, batch_size=64, shuffle=False)\n",
    "all_imgs_list, vis_labels, vis_preds, vis_probs = [], [], [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in vis_loader:\n",
    "        out = model(imgs.to(device))\n",
    "        prbs = torch.softmax(out,1)[:,1].cpu().numpy()\n",
    "        prds = out.argmax(1).cpu().numpy()\n",
    "        all_imgs_list.append(imgs)\n",
    "        vis_labels.extend(lbls.squeeze().numpy())\n",
    "        vis_preds.extend(prds); vis_probs.extend(prbs)\n",
    "all_imgs_t   = torch.cat(all_imgs_list, 0)\n",
    "vis_labels   = np.array(vis_labels)\n",
    "vis_preds    = np.array(vis_preds)\n",
    "vis_probs_np = np.array(vis_probs)\n",
    "\n",
    "fail_idx = np.where(vis_labels != vis_preds)[0]\n",
    "np.random.shuffle(fail_idx); sel = fail_idx[:16]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12,12))\n",
    "fig.suptitle(f'Failure Cases ‚Äî {len(fail_idx)}/{len(vis_labels)} misclassified', fontweight='bold')\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= len(sel): ax.axis('off'); continue\n",
    "    idx = sel[i]\n",
    "    img = all_imgs_t[idx].permute(1,2,0).numpy().clip(0,1)[:,:,0]\n",
    "    true_l = CLASS_NAMES[vis_labels[idx]]; pred_l = CLASS_NAMES[vis_preds[idx]]\n",
    "    conf = vis_probs_np[idx] if vis_preds[idx]==1 else 1-vis_probs_np[idx]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'True: {true_l}\\nPred: {pred_l} ({conf:.2f})', fontsize=8, color='#C62828')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/task1/failure_cases.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'‚úì {len(fail_idx)} failure cases ({100*len(fail_idx)/len(vis_labels):.1f}% error rate)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ Task 2: Medical Report Generation (MedGemma VLM)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ HuggingFace auth (required for MedGemma) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1. Accept terms: https://huggingface.co/google/medgemma-4b-it\n",
    "# 2. Paste your token below OR uncomment login()\n",
    "HF_TOKEN = ''  # <-- paste your HF token here\n",
    "if HF_TOKEN:\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "    print('‚úì HF_TOKEN set')\n",
    "else:\n",
    "    print('‚ö† No token ‚Äî will use structured mock reports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Three prompting strategies ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "PROMPTS = {\n",
    "    'concise': (\n",
    "        'You are a radiologist. In 2-3 sentences describe the key findings '\n",
    "        'and state whether this chest X-ray is normal or shows pneumonia.'\n",
    "    ),\n",
    "    'structured': (\n",
    "        'You are an expert radiologist. Provide a structured report:\\n'\n",
    "        '1. Lung Fields: Opacities, consolidation, infiltrates.\\n'\n",
    "        '2. Heart/Mediastinum: Size and contour.\\n'\n",
    "        '3. Impression: Normal or Pneumonia with brief justification.'\n",
    "    ),\n",
    "    'differential': (\n",
    "        'As a radiologist, describe visible lung field features, '\n",
    "        'give a differential diagnosis, then conclude with your '\n",
    "        'primary diagnosis: normal chest or pneumonia.'\n",
    "    ),\n",
    "}\n",
    "\n",
    "MOCK = {\n",
    "    ('Normal','concise'):\n",
    "        'The chest radiograph shows clear bilateral lung fields with no consolidation or infiltrate. '\n",
    "        'Cardiac silhouette is normal. Impression: Normal chest radiograph.',\n",
    "    ('Normal','structured'):\n",
    "        '1. Lung Fields: Clear bilaterally; no opacities, consolidation, or infiltrates. '\n",
    "        'Costophrenic angles sharp.\\n2. Heart/Mediastinum: Normal size; no widening.\\n'\n",
    "        '3. Impression: Normal chest radiograph ‚Äî no acute cardiopulmonary findings.',\n",
    "    ('Normal','differential'):\n",
    "        'Lung fields appear clear. Mild bronchovascular prominence noted but within limits. '\n",
    "        'Differential: (1) Normal, (2) Mild bronchitis. '\n",
    "        'Primary Diagnosis: Normal chest ‚Äî no evidence of pneumonia.',\n",
    "    ('Pneumonia','concise'):\n",
    "        'Increased right lower lobe opacity with air bronchograms consistent with consolidation. '\n",
    "        'Impression: Right lower lobe pneumonia.',\n",
    "    ('Pneumonia','structured'):\n",
    "        '1. Lung Fields: Right lower lobe consolidation with air bronchograms; '\n",
    "        'mild left perihilar haziness.\\n2. Heart/Mediastinum: Normal.\\n'\n",
    "        '3. Impression: Bilateral pneumonia, right > left. Clinical correlation recommended.',\n",
    "    ('Pneumonia','differential'):\n",
    "        'Right lower lobe focal consolidation with air bronchograms; subtle left lower haziness. '\n",
    "        'Differential: (1) CAP, (2) Aspiration pneumonitis. '\n",
    "        'Primary: Bacterial pneumonia ‚Äî bilateral, right predominant.',\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ Load MedGemma (best effort) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "vlm_model = vlm_processor = None\n",
    "USE_REAL_VLM = False\n",
    "try:\n",
    "    from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "    tok = os.environ.get('HF_TOKEN') or None\n",
    "    vlm_processor = AutoProcessor.from_pretrained('google/medgemma-4b-it', token=tok)\n",
    "    vlm_model = AutoModelForImageTextToText.from_pretrained(\n",
    "        'google/medgemma-4b-it', token=tok,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map='auto' if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    if not torch.cuda.is_available(): vlm_model = vlm_model.to(device)\n",
    "    vlm_model.eval(); USE_REAL_VLM = True\n",
    "    print('‚úì MedGemma-4B loaded!')\n",
    "except Exception as e:\n",
    "    print(f'MedGemma unavailable ({type(e).__name__}) ‚Üí using mock reports')\n",
    "    print('  To enable: accept terms at https://huggingface.co/google/medgemma-4b-it')\n",
    "    print('  and set HF_TOKEN above.')\n",
    "\n",
    "def tensor_to_pil(t):\n",
    "    img = t.permute(1,2,0).numpy().clip(0,1)[:,:,0]\n",
    "    return PILImage.fromarray((img*255).astype(np.uint8)).convert('RGB')\n",
    "\n",
    "def generate_report(pil_img, prompt, true_label, strategy):\n",
    "    if USE_REAL_VLM:\n",
    "        msgs = [{'role':'user','content':[{'type':'image','image':pil_img},{'type':'text','text':prompt}]}]\n",
    "        inp = vlm_processor.apply_chat_template(msgs, add_generation_prompt=True,\n",
    "                  tokenize=True, return_dict=True, return_tensors='pt').to(vlm_model.device)\n",
    "        with torch.no_grad():\n",
    "            out = vlm_model.generate(**inp, max_new_tokens=300, do_sample=False)\n",
    "        n = inp['input_ids'].shape[1]\n",
    "        return vlm_processor.decode(out[0][n:], skip_special_tokens=True).strip()\n",
    "    return MOCK.get((true_label, strategy), f'[Mock] {true_label} ‚Äì {strategy} strategy.')\n",
    "\n",
    "print('‚úì Report generation ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 Normal + 5 Pneumonia images for report generation\n",
    "tl_arr = np.array([test_ds[i][1].item() for i in range(len(test_ds))])\n",
    "np.random.seed(42)\n",
    "sel_idx = list(np.random.choice(np.where(tl_arr==0)[0], 5, replace=False)) + \\\n",
    "          list(np.random.choice(np.where(tl_arr==1)[0], 5, replace=False))\n",
    "\n",
    "prompt_keys = list(PROMPTS.keys())\n",
    "report_results = []\n",
    "\n",
    "for i, idx in enumerate(sel_idx):\n",
    "    img_t, lbl_t = vis_ds[idx]\n",
    "    true_lbl = CLASS_NAMES[lbl_t.item()]\n",
    "    strat    = prompt_keys[i % len(prompt_keys)]\n",
    "    t0 = time.time()\n",
    "    report = generate_report(tensor_to_pil(img_t), PROMPTS[strat], true_lbl, strat)\n",
    "    report_results.append({\n",
    "        'index': int(idx), 'true_label': true_lbl,\n",
    "        'cnn_pred': CLASS_NAMES[vis_preds[idx]],\n",
    "        'prompt_strategy': strat, 'report': report,\n",
    "        'generation_time_s': round(time.time()-t0, 2)\n",
    "    })\n",
    "    print(f'[{i+1}/10] idx={idx} True={true_lbl} CNN={CLASS_NAMES[vis_preds[idx]]} Strategy={strat}')\n",
    "\n",
    "with open('outputs/task2/generated_reports.json','w') as f:\n",
    "    json.dump(report_results, f, indent=2)\n",
    "print('\\n‚úì Saved generated_reports.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reports\n",
    "n = len(report_results)\n",
    "fig, axes = plt.subplots(n, 2, figsize=(16, 4.5*n))\n",
    "fig.suptitle('Generated Radiology Reports ‚Äî MedGemma', fontsize=14, fontweight='bold')\n",
    "for i, r in enumerate(report_results):\n",
    "    ax_i, ax_t = axes[i]\n",
    "    img_t, _ = vis_ds[r['index']]\n",
    "    ax_i.imshow(img_t.permute(1,2,0).numpy().clip(0,1)[:,:,0], cmap='gray')\n",
    "    match = r['true_label'] == r['cnn_pred']\n",
    "    ax_i.set_title(f\"True: {r['true_label']}  CNN: {r['cnn_pred']} {'‚úì' if match else '‚úó'}\",\n",
    "                   color='#2E7D32' if match else '#C62828', fontweight='bold')\n",
    "    ax_i.axis('off')\n",
    "    ax_t.axis('off')\n",
    "    ax_t.text(0.02, 0.97, f\"[{r['prompt_strategy']}]\\n\\n{r['report']}\",\n",
    "              transform=ax_t.transAxes, fontsize=8.5, va='top',\n",
    "              bbox=dict(boxstyle='round,pad=0.5', facecolor='#FFFDE7', alpha=0.85))\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/task2/reports_visualization.png', dpi=110, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('‚úì Saved reports_visualization.png')\n",
    "\n",
    "print(f'\\nReal VLM: {\"Yes (MedGemma-4B)\" if USE_REAL_VLM else \"Mock reports\"}')\n",
    "for r in report_results:\n",
    "    print(f'\\n‚îÄ‚îÄ {r[\"true_label\"]} | CNN:{r[\"cnn_pred\"]} | {r[\"prompt_strategy\"]} ‚îÄ‚îÄ')\n",
    "    print(r['report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Task 3: Semantic Image Retrieval (BioMedCLIP + FAISS)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip, faiss\n",
    "\n",
    "# Load best available medical embedding model\n",
    "emb_model = emb_preprocess = emb_tokenizer = None\n",
    "EMB_TYPE = None\n",
    "\n",
    "for model_id, pretrained, typ in [\n",
    "    ('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224', None, 'biomedclip'),\n",
    "    ('ViT-B-32', 'openai', 'clip'),\n",
    "]:\n",
    "    try:\n",
    "        emb_model, _, emb_preprocess = open_clip.create_model_and_transforms(\n",
    "            model_id, pretrained=pretrained)\n",
    "        emb_tokenizer = open_clip.get_tokenizer(model_id)\n",
    "        EMB_TYPE = typ\n",
    "        print(f'‚úì Loaded {typ}')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f'{typ} failed: {e}')\n",
    "\n",
    "if emb_model is None:\n",
    "    print('Using CNN (EfficientNet) features as fallback ‚Äî image-only search')\n",
    "    EMB_TYPE = 'cnn'\n",
    "else:\n",
    "    emb_model = emb_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "@torch.no_grad()\n",
    "def extract_embeddings_clip(ds, model, preprocess, bs=64):\n",
    "    embeddings, labels = [], []\n",
    "    for i in tqdm(range(0, len(ds), bs), desc='Embeddings'):\n",
    "        batch_imgs, batch_lbls = [], []\n",
    "        for j in range(i, min(i+bs, len(ds))):\n",
    "            t, l = ds[j]\n",
    "            np_img = t.permute(1,2,0).numpy().clip(0,1)[:,:,0]\n",
    "            pil = PILImage.fromarray((np_img*255).astype(np.uint8)).convert('RGB')\n",
    "            batch_imgs.append(preprocess(pil)); batch_lbls.append(l.item())\n",
    "        bt = torch.stack(batch_imgs).to(device)\n",
    "        e = model.encode_image(bt).float()\n",
    "        e = e / e.norm(dim=-1, keepdim=True)\n",
    "        embeddings.append(e.cpu().numpy()); labels.extend(batch_lbls)\n",
    "    return np.vstack(embeddings), np.array(labels)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings_cnn(ds, cnn, bs=64):\n",
    "    import timm\n",
    "    fe = timm.create_model('efficientnet_b0', pretrained=False, num_classes=0)\n",
    "    sd = {k:v for k,v in torch.load('outputs/task1/best_model.pth', map_location=device).items()\n",
    "          if 'classifier' not in k}\n",
    "    fe.load_state_dict(sd, strict=False); fe = fe.to(device).eval()\n",
    "    embeddings, labels = [], []\n",
    "    loader = DataLoader(ds, batch_size=bs, shuffle=False)\n",
    "    for imgs, lbls in tqdm(loader, desc='CNN embeddings'):\n",
    "        e = fe(imgs.to(device)).float()\n",
    "        e = e / (e.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        embeddings.append(e.cpu().numpy()); labels.extend(lbls.squeeze().numpy())\n",
    "    return np.vstack(embeddings), np.array(labels)\n",
    "\n",
    "if EMB_TYPE in ('biomedclip', 'clip'):\n",
    "    embeddings, emb_labels = extract_embeddings_clip(vis_ds, emb_model, emb_preprocess)\n",
    "else:\n",
    "    embeddings, emb_labels = extract_embeddings_cnn(vis_ds, model)\n",
    "\n",
    "np.save('outputs/task3/embeddings.npy', embeddings)\n",
    "np.save('outputs/task3/labels.npy', emb_labels)\n",
    "with open('outputs/task3/embedding_info.json','w') as f:\n",
    "    json.dump({'model_type': EMB_TYPE, 'embedding_dim': embeddings.shape[1],\n",
    "               'num_samples': len(emb_labels)}, f, indent=2)\n",
    "print(f'‚úì Embeddings: {embeddings.shape} | Model: {EMB_TYPE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index (IndexFlatIP = exact cosine sim on L2-normalized vectors)\n",
    "d = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(d)\n",
    "faiss_index.add(embeddings.astype(np.float32))\n",
    "faiss.write_index(faiss_index, 'outputs/task3/faiss_index.bin')\n",
    "print(f'‚úì FAISS index | {faiss_index.ntotal} vectors, dim={d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-to-image search demo\n",
    "def img2img(query_idx, k=5):\n",
    "    q = embeddings[query_idx].reshape(1,-1).astype(np.float32)\n",
    "    scores, idxs = faiss_index.search(q, k+1)\n",
    "    return [(int(i), float(s)) for i,s in zip(idxs[0], scores[0]) if i!=-1 and i!=query_idx][:k]\n",
    "\n",
    "def show_retrieval(q_idx, retrieved, title='', save=None):\n",
    "    k = len(retrieved)\n",
    "    fig, axes = plt.subplots(1, k+1, figsize=(3*(k+1), 3.8))\n",
    "    fig.suptitle(title, fontweight='bold')\n",
    "    def draw(ax, idx, lbl_str, color, bcolor=None):\n",
    "        img = vis_ds[idx][0].permute(1,2,0).numpy().clip(0,1)[:,:,0]\n",
    "        ax.imshow(img, cmap='gray'); ax.set_title(lbl_str, color=color, fontsize=8.5); ax.axis('off')\n",
    "        if bcolor:\n",
    "            for sp in ax.spines.values(): sp.set_visible(True); sp.set_color(bcolor); sp.set_linewidth(3)\n",
    "    draw(axes[0], q_idx, f'QUERY\\n{CLASS_NAMES[emb_labels[q_idx]]}', 'blue', 'blue')\n",
    "    for j, (ri, sc) in enumerate(retrieved):\n",
    "        match = emb_labels[ri] == emb_labels[q_idx]\n",
    "        draw(axes[j+1], ri, f'#{j+1} {CLASS_NAMES[emb_labels[ri]]}\\n{sc:.3f} {\"‚úì\" if match else \"‚úó\"}',\n",
    "             '#2E7D32' if match else '#C62828')\n",
    "    plt.tight_layout()\n",
    "    if save: plt.savefig(save, dpi=130, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "for q_idx, name in [(int(np.where(emb_labels==0)[0][5]), 'normal'),\n",
    "                     (int(np.where(emb_labels==1)[0][5]), 'pneumonia')]:\n",
    "    retrieved = img2img(q_idx, k=5)\n",
    "    show_retrieval(q_idx, retrieved,\n",
    "                   title=f'Image-to-Image: {CLASS_NAMES[emb_labels[q_idx]]} Query',\n",
    "                   save=f'outputs/task3/retrieval_{name}.png')\n",
    "    precision = np.mean([emb_labels[i]==emb_labels[q_idx] for i,_ in retrieved])\n",
    "    print(f'Query={CLASS_NAMES[emb_labels[q_idx]]} | P@5={precision:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-image search\n",
    "if emb_tokenizer is not None:\n",
    "    def txt2img(query_text, k=5):\n",
    "        with torch.no_grad():\n",
    "            toks = emb_tokenizer([query_text]).to(device)\n",
    "            te = emb_model.encode_text(toks).float()\n",
    "            te = te / te.norm(dim=-1, keepdim=True)\n",
    "        sc, ids = faiss_index.search(te.cpu().numpy().astype(np.float32), k)\n",
    "        return [(int(i), float(s)) for i,s in zip(ids[0], sc[0]) if i!=-1]\n",
    "\n",
    "    for q in ['bilateral lung consolidation pneumonia',\n",
    "              'clear normal lung fields no abnormality']:\n",
    "        res = txt2img(q, k=5)\n",
    "        lbls = [CLASS_NAMES[emb_labels[i]] for i,_ in res]\n",
    "        print(f'Text: \"{q}\"\\n  ‚Üí Retrieved labels: {lbls}\\n')\n",
    "else:\n",
    "    print('Text-to-image search unavailable (CNN fallback mode ‚Äî no text encoder)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision@k evaluation\n",
    "K_VALS, N_QUERIES = [1, 5, 10], 200\n",
    "np.random.seed(42)\n",
    "q_idxs = np.random.choice(len(embeddings), N_QUERIES, replace=False)\n",
    "pk_results = {k: [] for k in K_VALS}\n",
    "max_k = max(K_VALS)\n",
    "for qi in tqdm(q_idxs, desc='P@k eval'):\n",
    "    q = embeddings[qi].reshape(1,-1).astype(np.float32)\n",
    "    _, ids = faiss_index.search(q, max_k+1)\n",
    "    retrieved = [int(i) for i in ids[0] if i!=-1 and i!=qi]\n",
    "    for k in K_VALS:\n",
    "        top = retrieved[:k]\n",
    "        if top: pk_results[k].append(np.mean([emb_labels[i]==emb_labels[qi] for i in top]))\n",
    "\n",
    "precision_at_k = {f'P@{k}': float(np.mean(pk_results[k])) for k in K_VALS}\n",
    "baseline = float(max(np.mean(emb_labels==0), np.mean(emb_labels==1)))\n",
    "\n",
    "with open('outputs/task3/precision_at_k.json','w') as f:\n",
    "    json.dump(precision_at_k, f, indent=2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4.5))\n",
    "bars = ax.bar(list(precision_at_k.keys()), list(precision_at_k.values()),\n",
    "              color='#1565C0', width=0.5, edgecolor='white', zorder=3)\n",
    "ax.axhline(baseline, color='#EF5350', ls='--', lw=2, label=f'Baseline={baseline:.3f}')\n",
    "ax.set(xlabel='k', ylabel='Precision@k', title=f'Retrieval Precision@k ‚Äî {EMB_TYPE.upper()}')\n",
    "ax.set_ylim(0,1.05); ax.grid(axis='y',alpha=0.3,zorder=0); ax.legend()\n",
    "for bar, v in zip(bars, precision_at_k.values()):\n",
    "    ax.text(bar.get_x()+bar.get_width()/2, v+0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/task3/precision_at_k.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('=== PRECISION@K ===')\n",
    "for k, v in precision_at_k.items():\n",
    "    print(f'  {k}: {v:.4f}  (+{v-baseline:.4f} vs baseline)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\\n## üìä Final Summary"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('  COMPLETE SYSTEM SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "print('\\n‚îÄ‚îÄ Task 1: EfficientNet-B0 CNN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ')\n",
    "with open('outputs/task1/test_metrics.json') as f: m=json.load(f)\n",
    "for k,v in m.items(): print(f'  {k:10s}: {v:.4f}')\n",
    "\n",
    "print('\\n‚îÄ‚îÄ Task 2: MedGemma Report Generation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ')\n",
    "with open('outputs/task2/generated_reports.json') as f: rpts=json.load(f)\n",
    "print(f'  Reports: {len(rpts)} | Real VLM: {USE_REAL_VLM}')\n",
    "print(f'  Strategies: {\", \".join(set(r[\"prompt_strategy\"] for r in rpts))}')\n",
    "\n",
    "print('\\n‚îÄ‚îÄ Task 3: BioMedCLIP + FAISS Retrieval ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ')\n",
    "with open('outputs/task3/precision_at_k.json') as f: pk=json.load(f)\n",
    "print(f'  Model: {EMB_TYPE}')\n",
    "for k,v in pk.items(): print(f'  {k}: {v:.4f}')\n",
    "\n",
    "print('\\n‚úì All three tasks complete!')"
   ]
  }
 ]
}
